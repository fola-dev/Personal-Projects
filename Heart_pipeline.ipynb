{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.4.0\n",
      "  Downloading tensorflow-2.4.0-cp36-cp36m-manylinux2010_x86_64.whl (394.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 394.7 MB 1.6 MB/s eta 0:00:01     |███                             | 37.7 MB 1.6 MB/s eta 0:03:48     |███▉                            | 47.4 MB 1.6 MB/s eta 0:03:42     |███████▎                        | 90.2 MB 1.7 MB/s eta 0:03:03               | 122.1 MB 1.6 MB/s eta 0:02:54      | 144.6 MB 1.6 MB/s eta 0:02:40  | 202.0 MB 1.6 MB/s eta 0:02:04     |████████████████▋               | 205.3 MB 1.6 MB/s eta 0:02:01   | 239.5 MB 1.6 MB/s eta 0:01:40�████████▉           | 257.0 MB 1.4 MB/s eta 0:01:40�█████████           | 259.6 MB 1.4 MB/s eta 0:01:39     |█████████████████████▎          | 261.8 MB 1.4 MB/s eta 0:01:37     |██████████████████████▎         | 274.1 MB 1.4 MB/s eta 0:01:28��████████████████▌       | 302.6 MB 1.5 MB/s eta 0:01:03     |██████████████████████████▎     | 323.8 MB 1.5 MB/s eta 0:00:49 | 333.5 MB 1.5 MB/s eta 0:00:42Killed\n",
      "Collecting ipywidgets\n",
      "  Using cached ipywidgets-7.6.3-py2.py3-none-any.whl (121 kB)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (6.0.7)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets) (5.3.4)\n",
      "Collecting jupyterlab-widgets>=1.0.0; python_version >= \"3.6\"\n",
      "  Using cached jupyterlab_widgets-1.0.0-py3-none-any.whl (243 kB)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets) (5.0.8)\n",
      "Collecting widgetsnbextension~=3.5.0\n",
      "  Using cached widgetsnbextension-3.5.1-py2.py3-none-any.whl (2.2 MB)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets) (7.16.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets) (4.3.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.1.2)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.5.1)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert) (3.2.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (1.4.3)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (2.7.2)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.4.4)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.8.4)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.6.0)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert) (4.6.3)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.3)\n",
      "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (2.11.2)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.7)\n",
      "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets) (3.2.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.1.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (2.0.10)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (46.1.3)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.17.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.3.1->ipywidgets) (1.14.0)\n",
      "Requirement already satisfied: async-generator in /usr/local/lib/python3.6/dist-packages (from nbclient<0.6.0,>=0.5.0->nbconvert) (1.10)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.6/dist-packages (from nbclient<0.6.0,>=0.5.0->nbconvert) (1.4.3)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert) (0.5.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert) (20.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert) (1.1.1)\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (19.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.17.3)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (1.6.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (20.3.0)\n",
      "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.9.1)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.0)\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (20.1.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.6.0)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from jedi>=0.10->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->bleach->nbconvert) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.14.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.20)\n",
      "Installing collected packages: jupyterlab-widgets, widgetsnbextension, ipywidgets\n",
      "Successfully installed ipywidgets-7.6.3 jupyterlab-widgets-1.0.0 widgetsnbextension-3.5.1\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting pip\n",
      "  Downloading pip-21.0.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 46.8 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "\u001b[33m  WARNING: The scripts pip, pip3 and pip3.6 are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed pip-21.0.1\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.24.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.20.3)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.2.0-py3-none-any.whl (3.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7 MB 50.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.14.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.3)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.0)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-0.29.0-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 7.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting dataclasses\n",
      "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (20.3.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (2.22.0)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Collecting protobuf>=3.12.2\n",
      "  Downloading protobuf-3.15.8-cp36-cp36m-manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 74.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (1.1.0)\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (0.18.2)\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-5.1.2-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (0.9.0)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 15.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Using cached tqdm-4.60.0-py2.py3-none-any.whl (75 kB)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets) (1.24.2)\n",
      "Requirement already satisfied: zipp>=0.4 in /usr/local/lib/python3.6/dist-packages (from importlib-resources->tensorflow-datasets) (3.1.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.52.0)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21495 sha256=8195ec7fb1ec34b4c991d9595d3cc8e4cec7f00f03ac9c3424b16c0b5a5e1b78\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/59/9a/1d/3f1afbbb5122d0410547bf9eb50955f4a7a98e53a6d8b99bd1\n",
      "Successfully built promise\n",
      "Installing collected packages: protobuf, typing-extensions, tqdm, tensorflow-metadata, promise, importlib-resources, dill, dataclasses, tensorflow-datasets\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script tfds is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed dataclasses-0.8 dill-0.3.3 importlib-resources-5.1.2 promise-2.3 protobuf-3.15.8 tensorflow-datasets-4.2.0 tensorflow-metadata-0.29.0 tqdm-4.60.0 typing-extensions-3.7.4.3\n"
     ]
    }
   ],
   "source": [
    "#! pip3 list | grep tensorflow \n",
    "! pip3 install --user tensorflow==2.4.0\n",
    "! pip3 install --user ipywidgets nbconvert\n",
    "!python -m pip install --user --upgrade pip\n",
    "!pip3 install pandas scikit-learn keras tensorflow-datasets --user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER_FILE = \"tfjobheart.py\"\n",
    "KUBERNETES_FILE = \"tfjob-heartdisease.yaml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "from IPython.utils.capture import CapturedIO\n",
    "\n",
    "\n",
    "def get_resource(captured_io: CapturedIO) -> str:\n",
    "    \"\"\"\n",
    "    Gets a resource name from `kubectl apply -f <configuration.yaml>`.\n",
    "\n",
    "    :param str captured_io: Output captured by using `%%capture` cell magic\n",
    "    :return: Name of the Kubernetes resource\n",
    "    :rtype: str\n",
    "    :raises Exception: if the resource could not be created\n",
    "    \"\"\"\n",
    "    out = captured_io.stdout\n",
    "    matches = re.search(r\"^(.+)\\s+created\", out)\n",
    "    if matches is not None:\n",
    "        return matches.group(1)\n",
    "    else:\n",
    "        raise Exception(f\"Cannot get resource as its creation failed: {out}. It may already exist.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as  pd\n",
    "#data = pd.read_csv(\"heart.csv\")\n",
    "#data.head()\n",
    "file =\"heart.csv\"\n",
    "df= pd.read_csv(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tfjobheart.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $TRAINER_FILE\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from numpy.random import seed\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(221)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "def make_datasets_unbatched():\n",
    "    data = pd.read_csv(\"heart.csv\")\n",
    "    data.head()\n",
    "    \n",
    "    data.apply(lambda x: sum(x.isnull()),axis=0)\n",
    "    \n",
    "    # List of variables with missing values\n",
    "    \n",
    "    vars_with_na=[var for var in data.columns if data[var].isnull().sum()>1]\n",
    "#remove outliers\n",
    "    \n",
    "    def removeOutlier(att, data):\n",
    "        lowerbound = att.mean() - 3 * att.std()\n",
    "        upperbound = att.mean() + 3 * att.std()\n",
    "        #print('lowerbound: ',lowerbound,' -------- upperbound: ', upperbound )\n",
    "        df1 = data[(att > lowerbound) & (att < upperbound)]\n",
    "        #print((data.shape[0] - df1.shape[0]), ' number of outliers from ', data.shape[0] )\n",
    "        #print(' ******************************************************')\n",
    "        data = df1.copy()\n",
    "        return data\n",
    "    data = removeOutlier(data.trtbps, data)\n",
    "    data = removeOutlier(data.chol, data)\n",
    "    \n",
    "    #resampling\n",
    "    from sklearn.utils import resample\n",
    "    \n",
    "    # Separate Target Classes\n",
    "    df_1 = data[data.output==1]\n",
    "    df_2 = data[data.output==0]\n",
    "    \n",
    "    # Upsample minority class\n",
    "    df_upsample_1 = resample(df_2, \n",
    "     replace=True,     # sample with replacement\n",
    "                                 n_samples=163,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    "    # Combine majority class with upsampled minority class\n",
    "    df_upsampled = pd.concat([df_1, df_upsample_1])\n",
    "    \n",
    "    # Display new class counts\n",
    "    df_upsampled.output.value_counts()\n",
    "\n",
    "    x = df_upsampled.drop('output', axis = 1)\n",
    "    y = df_upsampled['output']\n",
    "    \n",
    "    #Split dataset\n",
    "\n",
    "    \n",
    "    x_train,x_test, y_train, y_test = tts(x,y, test_size = 0.2, random_state = 111)\n",
    "    \n",
    "    #Scaling\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.fit_transform(x_test)\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    train = train_dataset.cache().shuffle(2000).repeat()\n",
    "    return train, test_dataset\n",
    "\n",
    "def model(args):\n",
    "    seed(1)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_dim=13))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.summary()\n",
    "    opt = args.optimizer\n",
    "    model.compile(optimizer=opt,\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    tf.keras.backend.set_value(model.optimizer.learning_rate, args.learning_rate)\n",
    "    return model\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # MultiWorkerMirroredStrategy creates copies of all variables in the model's\n",
    "    # layers on each device across all workers\n",
    "    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(\n",
    "        communication=tf.distribute.experimental.CollectiveCommunication.AUTO)\n",
    "    logging.debug(f\"num_replicas_in_sync: {strategy.num_replicas_in_sync}\")\n",
    "    BATCH_SIZE_PER_REPLICA = args.batch_size\n",
    "    BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "    \n",
    "    # Datasets need to be created after instantiation of `MultiWorkerMirroredStrategy`\n",
    "    train_dataset, test_dataset = make_datasets_unbatched()\n",
    "    train_dataset = train_dataset.batch(batch_size=BATCH_SIZE)\n",
    "    test_dataset = test_dataset.batch(batch_size=BATCH_SIZE)\n",
    "\n",
    "    # See: https://www.tensorflow.org/api_docs/python/tf/data/experimental/DistributeOptions\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = \\\n",
    "    tf.data.experimental.AutoShardPolicy.DATA\n",
    "    \n",
    "    train_datasets_sharded  = train_dataset.with_options(options)\n",
    "    test_dataset_sharded = test_dataset.with_options(options)\n",
    "    \n",
    "    with strategy.scope():\n",
    "        # Model building/compiling need to be within `strategy.scope()`.\n",
    "        multi_worker_model = model(args)\n",
    "        # Keras' `model.fit()` trains the model with specified number of epochs and\n",
    "        # number of steps per epoch. \n",
    "        multi_worker_model.fit(train_datasets_sharded,\n",
    "                         epochs=50,\n",
    "                         steps_per_epoch=30)\n",
    "\n",
    "        eval_loss, eval_acc = multi_worker_model.evaluate(test_dataset_sharded, \n",
    "                                                    verbose=0, steps=10)\n",
    "        # Log metrics for Katib\n",
    "logging.info(\"loss={:.4f}\".format(eval_loss))\n",
    "        logging.info(\"accuracy={:.4f}\".format(eval_acc))\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--batch_size\",\n",
    "                      type=int,\n",
    "                      default=32,\n",
    "                      metavar=\"N\",\n",
    "                      help=\"Batch size for training (default: 128)\")\n",
    "    parser.add_argument(\"--learning_rate\", \n",
    "                      type=float,  \n",
    "                      default=0.1,\n",
    "                      metavar=\"N\",\n",
    "                      help='Initial learning rate')\n",
    "    parser.add_argument(\"--optimizer\", \n",
    "                      type=str, \n",
    "                      default='adam',\n",
    "                      metavar=\"N\",\n",
    "                      help='optimizer')\n",
    "    parsed_args, _ = parser.parse_known_args()\n",
    "    main(parsed_args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the docker file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM python:3.8\n",
    "WORKDIR /schema\n",
    "RUN pip install -U joblib pandas tensorflow-data-validation\n",
    "RUN pip install -U joblib pandas tensorflow-data-validation ipython tensorflow \n",
    "COPY schemag.py /schema\n",
    "ENTRYPOINT [\"python\", \"schemag.py\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FROM tensorflow/tensorflow:2.4.0\n",
    "RUN pip install tensorflow_datasets pandas scikit-learn keras\n",
    "COPY tfjobheartdisease.py /\n",
    "ENTRYPOINT [\"python\", \"/tfjobheart.py\", \"--batch_size\", \"64\", \"--learning_rate\", \"0.1\", \"--optimizer\", \"adam\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run $TRAINER_FILE --optimizer 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "%%writefile $KUBERNETES_FILE\n",
    "apiVersion: \"kubeflow.org/v1\"\n",
    "kind: \"TFJob\"\n",
    "metadata:\n",
    "  name: \"hrtd\"\n",
    "  namespace: ekemini # your-user-namespace\n",
    "spec:\n",
    "  cleanPodPolicy: None\n",
    "  tfReplicaSpecs:\n",
    "    Worker:\n",
    "      replicas: 2\n",
    "      restartPolicy: OnFailure\n",
    "      template:\n",
    "        metadata:\n",
    "          annotations:\n",
    "            sidecar.istio.io/inject: \"false\"\n",
    "        spec:\n",
    "          containers:\n",
    "          - name: tensorflow\n",
    "            # modify this property if you would like to use a custom image\n",
    "            image: mavencodevv/tfjob_heart:v.0.1\n",
    "            command:\n",
    "                - \"python\"\n",
    "                - \"/tfjobheart.py\"\n",
    "                - \"--batch_size=64\"\n",
    "                - \"--learning_rate=0.1\"\n",
    "                - \"--optimizer=adam\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%capture tf_output --no-stderr\n",
    "! kubectl create -f $KUBERNETES_FILE\n",
    "TF_JOB = get_resource(tf_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl describe $TF_JOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl get pods -l job-name=hrtd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl logs -f hrtd-worker-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl delete $TF_JOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! kubectl -n ekemini logs -f hrtd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
